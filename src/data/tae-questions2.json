{
  "exam_info": {
    "title": "ISTQB Certified Tester Advanced Level - Test Automation Engineer",
    "version": "2.0",
    "model": "A",
    "date": "2024-05-03",
    "total_questions": 40,
    "passing_score": 65
  },
  "questions": [
    {
      "question_number": 1,
      "points": 1,
      "learning_objective": "TAE-1.1.1",
      "level": "K2",
      "question_text": "Which of the following is a limitation of test automation?",
      "options": {
        "a": "Usability tests can be automated effectively",
        "b": "Test automation can be executed only after the system under test (SUT) is implemented and deployed",
        "c": "Test automation can only check results that can be verified visually",
        "d": "Test automation can only check results that can be verified by code"
      },
      "correct_answer": "d",
      "explanation": "a) Is not correct. Test automation can be used for usability testing effectively. b) Is not correct. Test automation can be performed at each test level. c) Is not correct. Test automation is generally not effective to visually evaluate the test results. d) Is correct. See Chapter 1.1.1 of the syllabus."
    },
    {
      "question_number": 2,
      "points": 1,
      "learning_objective": "TAE-1.2.1",
      "level": "K2",
      "question_text": "Which of the following is true about test automation and the SDLC?",
      "options": {
        "a": "In Agile software development automated tests focus more on acceptance tests than on component tests",
        "b": "In Agile software development automated tests focus more on component tests than on acceptance tests",
        "c": "In the V-model automated test execution must be performed after manual test execution",
        "d": "In the V-model implementation of test automation is performed throughout the whole software development cycle"
      },
      "correct_answer": "b",
      "explanation": "a) Is not correct. In Agile software development, based on the test pyramid, automated component tests are higher in volume compared to automated acceptance tests. b) Is correct. In Agile software development, based on the test pyramid, automated component tests are higher in volume compared to automated acceptance tests. c) Is not correct. There is no such rule in the V-model. d) Is not correct. V-model test planning, including planning of test automation, is performed in the early phase of the software development lifecycle."
    },
    {
      "question_number": 3,
      "points": 1,
      "learning_objective": "TAE-1.2.2",
      "level": "K2",
      "question_text": "Which one of the following factors is NOT necessary to consider when determining suitable test tools?",
      "options": {
        "a": "SUT architecture",
        "b": "Actual composition and experience of the test team",
        "c": "Licensing and support of the test tool",
        "d": "Quality of the SUT requirements"
      },
      "correct_answer": "d",
      "explanation": "a) Is not correct. The SUT architecture should be considered. b) Is not correct. The actual composition and experience of the test team needs to be evaluated. c) Is not correct. It is necessary to have knowledge about the licensing and support of the tool. d) Is correct. It is not required to apply only one test tool for test automation."
    },
    {
      "question_number": 4,
      "points": 1,
      "learning_objective": "TAE-2.1.1",
      "level": "K2",
      "question_text": "When a system is designed for testability, one of the characteristics is that the test automation framework (TAF) can access interfaces to perform actions on the system. What is this characteristic called?",
      "options": {
        "a": "Observability",
        "b": "Controllability",
        "c": "Maintainability",
        "d": "Interoperability"
      },
      "correct_answer": "b",
      "explanation": "a) Is not correct. Observability means to give insight into the system. b) Is correct. See Chapter 2.1.1 of the syllabus. c) Is not correct. Maintainability is a quality characteristic. d) Is not correct. Interoperability is also a quality characteristic."
    },
    {
      "question_number": 5,
      "points": 1,
      "learning_objective": "TAE-2.1.2",
      "level": "K2",
      "question_text": "What type of test automation is mainly performed in the Staging environment?",
      "options": {
        "a": "Component testing",
        "b": "Performance efficiency testing and user acceptance testing",
        "c": "Static analysis",
        "d": "Integration testing"
      },
      "correct_answer": "b",
      "explanation": "a) Is not correct. Component testing is performed in a build and development environment. b) Is correct. It is necessary to perform performance efficiency testing and user acceptance testing in a preproduction environment to test real world scenarios. c) Is not correct. Smoke testing can be performed in any test environment. d) Is not correct. Tests in a preproduction environment assess components availability in the development environment."
    },
    {
      "question_number": 6,
      "points": 1,
      "learning_objective": "TAE-2.1.2",
      "level": "K2",
      "question_text": "In which environment is a fully automated test suite typically executed against a release candidate for the first time?",
      "options": {
        "a": "Preproduction/staging environment",
        "b": "Build environment",
        "c": "Production/operational environment",
        "d": "Integration/test environment"
      },
      "correct_answer": "d",
      "explanation": "a) Is not correct. Preproduction or staging environment testing has less focus on functional aspects. b) Is not correct. A build environment is not ready for more robust test execution. c) Is not correct. Full functional test suites are not typically executed against products in production. d) Is correct. Robust user interface (UI), and application programming interface (API) test suites are typically executed against fully integrated systems."
    },
    {
      "question_number": 7,
      "points": 3,
      "learning_objective": "TAE-2.2.1",
      "level": "K4",
      "question_text": "You are working for an IT company which is developing a built-in Android-based car multimedia system. The software contains several components working together. Developers are following the test-driven development approach. After the development of the software, it is delivered to another IT company which integrates the software with the hardware elements and sells them together to car manufacturers. Which of the following should be considered during capturing the test automation requirements?",
      "options": {
        "a": "Is it important for the test automation approach to support component testing?",
        "b": "Should the test automation approach support beta testing?",
        "c": "Is it important for the test automation approach to support the testing of the software in as many different types of cars as possible?",
        "d": "Which tester roles should be supported by the test automation approach?",
        "e": "Is it important for the test automation approach to support the mobile application store approval?"
      },
      "correct_answer": ["a", "d"],
      "explanation": "a) Is correct. Component testing is performed by the developing IT company. b) Is not correct. Beta testing is not performed. c) Is not correct. The testing of as many different types of cars as possible is performed by the integrator IT company. d) Is correct. Automated component tests are designed and executed by the developers. e) Is not correct. The test automation approach to support the mobile application store approval is not performed."
    },
    {
      "question_number": 8,
      "points": 3,
      "learning_objective": "TAE-2.2.2",
      "level": "K4",
      "question_text": "You are evaluating test automation tools. One of the tools has a very informative dashboard which shows all the relevant test information about the SUT. Also, the tool includes a test logging component which logs all the necessary information that follows test execution and to troubleshoot problems found during the tests. It also contains a customizable test reporting component. During the proof of concept, the tool performed very slowly against the SUT. The current test environment is valid according to the release notes of the tool which means it fulfills the hardware and software requirements. What should be your next step regarding the selection of this tool?",
      "options": {
        "a": "Acquire more hardware resources for the SUT to decrease the performance degradation",
        "b": "Turn off test logging to improve performance of the tool",
        "c": "Recommend not selecting this tool",
        "d": "Plan to migrate the SUT to another hardware/software environment where there is a possibility for the elimination of the tool overhead"
      },
      "correct_answer": "c",
      "explanation": "a) Is not correct. The test environment is valid according to the release notes. b) Is not correct. The test logging component is essential for troubleshooting. c) Is correct. The tool does not meet selection requirements. d) Is not correct. It is completely wrong to migrate the SUT."
    },
    {
      "question_number": 9,
      "points": 1,
      "learning_objective": "TAE-3.1.1",
      "level": "K2",
      "question_text": "Which of the following is NOT a principle that supports easy development, evolution, and maintenance of the TAS?",
      "options": {
        "a": "The components of a TAS must depend on abstractions rather than on low level details",
        "b": "Every TAS component must be open for extension but closed for modification",
        "c": "Every TAS component must have an abstract, higher level parent component",
        "d": "Every TAS component must have a single responsibility"
      },
      "correct_answer": "c",
      "explanation": "a) Is not correct. This is a principle that supports easy maintenance. b) Is not correct. This is the open-closed principle. c) Is correct. This is not a principle which is stated in Chapter 3.1.1 of the syllabus. d) Is not correct. This is the single responsibility principle."
    },
    {
      "question_number": 10,
      "points": 1,
      "learning_objective": "TAE-3.1.2",
      "level": "K2",
      "question_text": "Which technical design aspect does not address the overall Test Automation Solution?",
      "options": {
        "a": "Selecting test automation tools",
        "b": "Developing user stories",
        "c": "Identifying interface requirements",
        "d": "Utilizing a version control system"
      },
      "correct_answer": "b",
      "explanation": "a) Is not correct. This is defined by the test automation architecture. b) Is correct. User stories are not a part of the technical design of the test automation architecture. c) Is not correct. This is defined by the test automation architecture. d) Is not correct. This is defined by the test automation architecture."
    },
    {
      "question_number": 11,
      "points": 2,
      "learning_objective": "TAE-3.1.3",
      "level": "K3",
      "question_text": "You are working in an Android development team and have been maintaining a test automation framework. An additional development team has formed to build a new application. Your project manager asks you to build a test automation framework for this newly formed team. At first, you identify the components that could be reused in building that new framework, then start implementing the additional libraries based on the new application. In which layer do you configure the connection to the new app?",
      "options": {
        "a": "Core libraries layer",
        "b": "Test scripts layer",
        "c": "Feature files layer",
        "d": "Business logic layer"
      },
      "correct_answer": "d",
      "explanation": "a) Is not correct. Core libraries should be application independent and generic. b) Is not correct. Test scripts should only contain test scripts and no configuration. c) Is not correct. Feature files contain scenarios written in the Gherkin language. d) Is correct. The business logic layer is used to set up the TAF to run against the SUT."
    },
    {
      "question_number": 12,
      "points": 2,
      "learning_objective": "TAE-3.1.4",
      "level": "K3",
      "question_text": "You are working on a test automation project that is used to automate GUI testing of a web-based public transport service. The project has a limited timescale. There are manual test cases which can be automated first. One of the goals is to implement test cases directly into the automated test scripts. Which technique or approach should be used for automating test cases to meet the goals?",
      "options": {
        "a": "Using the keyword-driven test technique",
        "b": "Using the behavior-driven development approach",
        "c": "Using the capture/playback test automation approach",
        "d": "Using the data-driven test automation technique"
      },
      "correct_answer": "c",
      "explanation": "a) Is not correct. Keyword-driven testing is too complex for this solution. b) Is not correct. Behavior-driven development is too complex and is not necessary. c) Is correct. Capture/playback test automation is easy to set up. d) Is not correct. Data-driven test automation is too complex and is not necessary."
    },
    {
      "question_number": 13,
      "points": 2,
      "learning_objective": "TAE-3.1.4",
      "level": "K3",
      "question_text": "You join a company where manual testing is mature, but test automation has been abandoned for a while now. The testers have generated a massive amount of test data and are typically using 10 to 20 variations per scenario. After your initial review, you see that the TAF can easily be fixed, but the test cases need to be completely revamped. Which of the following test automation approaches should you choose to achieve great results quickly?",
      "options": {
        "a": "Data-driven testing",
        "b": "Behavior-driven development",
        "c": "Capture/playback",
        "d": "Acceptance test-driven development"
      },
      "correct_answer": "a",
      "explanation": "a) Is correct. The test data is already available and can be reused to increase the test case count. b) Is not correct. Behavior-driven development requires involvement from business representatives, and in this case, there is no specific mention whether the business is involved. c) Is not correct. Capture/playback would be a slow and costly solution, while the test data is already present. d) Is not correct. Acceptance test-driven development needs involvement from all stakeholders, and in this case, there is no specific mention of those stakeholders' involvement."
    },
    {
      "question_number": 14,
      "points": 2,
      "learning_objective": "TAE-3.1.5",
      "level": "K3",
      "question_text": "You are working on a test automation project that is used to automate GUI testing of an e-commerce site. The site contains a digital assistant which helps users to set up their accounts, their name, billing address, shipping address, and security credentials. Currently, the development of the software is in a phase where usability testers check the digital assistant and give recommendations on necessary changes. This is done iteratively; the developers modify the graphical user interface (GUI), and the usability testers check the modifications, and do usability testing again. The test automation is mainly focused on supporting maintenance testing. Which design pattern is the best implemented in this case?",
      "options": {
        "a": "Implement the page object pattern and store all the user actions associated with the GUI elements in the relevant page models",
        "b": "Implement the flow model pattern, store all the web elements in the relevant page models, and store all the user actions associated with the GUI elements in the relevant flow models",
        "c": "Implement the facade design pattern and provide interfaces for the GUI elements to hide the used internal locator mechanism",
        "d": "Implement the singleton design pattern and create a single piece of code to handle the locating of elements"
      },
      "correct_answer": "b",
      "explanation": "a) Is not correct. Storing the user actions in the page model class files will not allow easy reuse and maintainability of complex user flows. b) Is correct. The flow model pattern combines usage of page models and flow models, storing the structure and logic of the application in class files with different purposes, and is typically used when the SUT's structure changes frequently. c) Is not correct. Facades provide a simplified interface to a larger body of code. d) Is not correct. The singleton design pattern does not solve the problem in this case."
    },
    {
      "question_number": 15,
      "points": 2,
      "learning_objective": "TAE-4.1.1",
      "level": "K3",
      "question_text": "The senior management wants to implement a TAS in your organization and asks you to lead this initiative. You have been given directions to start a pilot project. Which of the following statements best describes the objective of this pilot project? i. Document the SUT parts which have not been documented during the development ii. Identify the metrics and the measurement methods to monitor the SUT in the production environment iii. Analyze defects found during the testing of the TAS iv. Evaluate licensing options and corporation rules v. Select the most suitable commercial off-the-shelf or open-source tool",
      "options": {
        "a": "i, ii and iii are valid objectives of the pilot project",
        "b": "ii and iv are valid objectives of the pilot project",
        "c": "i, ii and v are valid objectives of the pilot project",
        "d": "iv and v are valid objectives of the pilot project"
      },
      "correct_answer": "d",
      "explanation": "a) Is not correct. i, ii and iii are not valid objectives of the pilot project. b) Is not correct. ii is not a valid objective of the pilot project. c) Is not correct. i and ii are not valid objectives of the pilot project. d) Is correct. iv and v are valid objectives of the pilot project."
    },
    {
      "question_number": 16,
      "points": 3,
      "learning_objective": "TAE-4.2.1",
      "level": "K4",
      "question_text": "There is an IT company which develops an often-changing financial software product using the Agile software development model. The development, the integration and the deployment processes are highly automated. You are working on a TAS which contains functional suitability tests which must be executed automatically after each daily build. Pair up the deployment risks below with the relevant mitigation strategies: 1. Test execution is not triggered by the build 2. Only the full test suite can be executed 3. Test data is not available when starting the test 4. It is not easy to troubleshoot failed tests A. Log detailed information during test execution B. Integrate test automation into the continuous integration/continuous delivery (CI/CD) pipeline C. Use third-party tools to generate test data D. Utilize test harnesses and test fixtures",
      "options": {
        "a": "1-C, 2-B, 3-D, 4-A",
        "b": "1-A, 2-B, 3-C, 4-D",
        "c": "1-B, 2-D, 3-C, 4-A",
        "d": "1-D, 2-B, 3-C, 4-A"
      },
      "correct_answer": "c",
      "explanation": "a) Is not correct. Triggering test execution by the build can be a risk to the CI/CD pipeline integration. b) Is not correct. Test logging detailed information supports easy troubleshooting of failed tests. c) Is correct. d) Is not correct. Risk related to test data cannot be mitigated by using third-party tools."
    },
    {
      "question_number": 17,
      "points": 1,
      "learning_objective": "TAE-4.3.1",
      "level": "K2",
      "question_text": "Which one of the following is an important factor to improve code maintainability?",
      "options": {
        "a": "Define generic functions with all the necessary parameters",
        "b": "Let developers uniquely name code variables",
        "c": "Use static analyzers to keep the code clean",
        "d": "Hardcode values to easily understand their meaning"
      },
      "correct_answer": "c",
      "explanation": "a) Is not correct. This answer does not improve code maintainability. b) Is not correct. Uniquely naming code variables does not improve code maintainability. c) Is correct. Using static analyzers improves code maintainability. d) Is not correct. Hard coding values does not improve code maintainability."
    },
    {
      "question_number": 18,
      "points": 1,
      "learning_objective": "TAE-4.3.1",
      "level": "K2",
      "question_text": "What is the best approach to reduce the maintenance time required for the test automation code?",
      "options": {
        "a": "Store the code outside a configuration management system",
        "b": "Hardcode all data",
        "c": "Leverage design patterns",
        "d": "Use a configuration management system"
      },
      "correct_answer": "c",
      "explanation": "a) Is not correct. Storing the code outside of a configuration management system makes the work harder for the test automation engineers (TAEs). b) Is not correct. Hard coding of any data is not recommended. c) Is correct. Structuring the code with design patterns can reduce maintenance time. d) Is not correct. Using a configuration management system still requires that rules are followed."
    },
    {
      "question_number": 19,
      "points": 2,
      "learning_objective": "TAE-5.1.1",
      "level": "K3",
      "question_text": "There is an IT company which develops an often-changing financial software product using the Agile software development model. The development, the integration and the deployment processes are highly automated. There is a CI/CD pipeline currently established. You are working on a TAS. The goal is to create a TAS which can be used for as many test automation purposes as possible. Which of the following options are valid purposes for test automation in this case?",
      "options": {
        "a": "Run a regression test suite every night",
        "b": "Execute a build of a component",
        "c": "Run a static code analysis",
        "d": "Execute an automated performance efficiency test in the CI/CD pipeline",
        "e": "Package and deploy the application as part of the deployment phase"
      },
      "correct_answer": ["a", "d"],
      "explanation": "a) Is correct. Regression testing is necessary, and it can be incorporated into the CI/CD pipeline. b) Is not correct. Executing a build is not a test automation task. c) Is not correct. Static code analysis is not a test automation task. d) Is correct. Performance efficiency tests can be automated. e) Is not correct. Packaging and deployment are not test automation tasks."
    },
    {
      "question_number": 20,
      "points": 2,
      "learning_objective": "TAE-5.1.1",
      "level": "K3",
      "question_text": "Which statement is correct?",
      "options": {
        "a": "Tests are not executed as part of the deployment phase",
        "b": "Tests are not executed as a separate pipeline, triggered by the successful deployment",
        "c": "Test cases do not act as a quality gate when different automated test suites will run on each deployment",
        "d": "Test in a preproduction environment can be used to ensure the SUT functionality"
      },
      "correct_answer": "c",
      "explanation": "a) Is not correct. Tests are executed during deployment. b) Is not correct. Tests are triggered to execute after successful deployment. c) Is correct. It is not a correct statement since tests do not act as quality gates for deployment. d) Is not correct. Tests in a preproduction environment verify SUT deployment, not SUT functionality."
    },
    {
      "question_number": 21,
      "points": 1,
      "learning_objective": "TAE-5.1.2",
      "level": "K2",
      "question_text": "How is configuration management used in test automation?",
      "options": {
        "a": "It enables the management of test data and test environment configurations",
        "b": "The SUT configuration can be stored and easily removed",
        "c": "It enables management of user rights for accessing test automation",
        "d": "Test automation results can easily be analyzed"
      },
      "correct_answer": "a",
      "explanation": "a) Is correct. Test data and test environment configurations can be under configuration management. b) Is not correct. The SUT configuration can be under configuration management, but it is not related to test automation. c) Is not correct. User rights management is not related to configuration management. d) Is not correct. Configuration management does not support test automation results analysis."
    },
    {
      "question_number": 22,
      "points": 1,
      "learning_objective": "TAE-5.1.2",
      "level": "K2",
      "question_text": "Which item below is NOT part of the test environment configuration?",
      "options": {
        "a": "Uniform resource locators (URLs)",
        "b": "Credentials",
        "c": "Test data",
        "d": "Common core library"
      },
      "correct_answer": "d",
      "explanation": "a) Is not correct. URLs are part of the test environment configuration. b) Is not correct. Credentials are part of the test environment configuration. c) Is not correct. Test data is part of the test environment configuration. d) Is correct. The test environment configuration is a part of the common core library, not vice versa."
    },
    {
      "question_number": 23,
      "points": 1,
      "learning_objective": "TAE-5.1.3",
      "level": "K2",
      "question_text": "Which of the following is NOT true for contract testing?",
      "options": {
        "a": "It is a lightweight form of API testing",
        "b": "Can be used to test the communication of microservices",
        "c": "Validates the compatibility of two separate systems",
        "d": "Verifies whether a system satisfies its contractual requirements"
      },
      "correct_answer": "d",
      "explanation": "a) Is not correct. Contract testing is the lightweight form of API testing. b) Is not correct. Contract testing can be used to test communication between microservices. c) Is not correct. Contract testing can validate the compatibility of two separate systems. d) Is correct. Contract testing has no relation to the contractual requirements."
    },
    {
      "question_number": 24,
      "points": 1,
      "learning_objective": "TAE-5.1.3",
      "level": "K2",
      "question_text": "You are on a project where the teams are working on breaking down an old monolithic web service into several microservices. Which documents can help to build your TAS? i. Application programming interface (API) specification ii. System architecture diagram iii. Test strategy iv. Release notes",
      "options": {
        "a": "i, ii, and iv",
        "b": "i and ii",
        "c": "ii, iii, and iv",
        "d": "i"
      },
      "correct_answer": "b",
      "explanation": "a) Is not correct. Release notes do not help to learn the API connections and details. b) Is correct. We can learn from the API specification about the details of the endpoints and from the system architecture diagram we can learn about the different background dependencies. c) Is not correct. The test strategy and release notes hold no helpful information about the APIs. d) Is not correct. A system architecture diagram is useful to build up the solution, but this answer is missing the API specification, so it is only partially correct."
    },
    {
      "question_number": 25,
      "points": 2,
      "learning_objective": "TAE-6.1.1",
      "level": "K3",
      "question_text": "You are working on a test automation project that is used to automate GUI testing of an online web shop. The web shop contains a wizard which helps users to set up their accounts, their name, billing address, shipping address, and security credentials. During the test automation, the steps of the wizard are recorded first. Screenshots are taken and stored during these steps. We consider these as the baseline. The baseline was then rerun with no change to the SUT and all tests passed. After the developers submit a change to the wizard, the recorded test scripts are played back, and the screenshots of each step are compared with the baseline screenshots. During a playback all previous test cases fail. You examine the test results in detail. What is the main reason for this experienced behavior?",
      "options": {
        "a": "An internally used technical session ID is also recorded, which changes during the playback. This should be substituted with variables",
        "b": "The screenshots are not properly linked to the steps and are therefore compared in the wrong order",
        "c": "Comparing screenshots cannot give a reliable test result, so another method must be used to evaluate the test result",
        "d": "The date in the GUI header is different from the date when it was recorded. This date field should be removed from the comparison",
        "e": "Screenshots are captured with low resolution; the details are blurred, and the comparison cannot give reliable results in this case"
      },
      "correct_answer": ["a", "d"],
      "explanation": "a) Is correct. This can be a reason for the experienced behavior. b) Is not correct. The order of the comparison is not relevant. c) Is not correct. This is a false statement. d) Is correct. See Chapter 6.1.1 of the syllabus. e) Is not correct. Taking screenshots in low resolution cannot be the reason for this behavior."
    },
    {
      "question_number": 26,
      "points": 2,
      "learning_objective": "TAE-6.1.1",
      "level": "K3",
      "question_text": "You are working on a project where you are responsible to extend the current TAF that is used for web service testing, with additional test logging capabilities. The TAF uses a third-party tool to create file logs and an HTML report to quickly visualize the test results. In the test implementation, several dynamic values are used for generating the actual test data and the SUT connects to several legacy test systems. Unfortunately, the tests are very unstable, and you need to add meaningful information to the test logging to better analyze the reasons for the failures. What additional information should not be included in the test logging to make it more useful and why?",
      "options": {
        "a": "Timestamps should be included in the test logs to see if the failure connects to a given legacy system outage",
        "b": "Screenshots should be included to see actual request-responses",
        "c": "The randomly generated values should be logged to see the actual results that were used during test executions",
        "d": "In case of assertion failures meaningful information like actual results versus expected results should be logged instead of stack traces"
      },
      "correct_answer": "b",
      "explanation": "a) Is not correct. The timestamps can help see when there are problems and if they are connected to a legacy service outage that affects the functionality of the SUT. b) Is correct. The SUT is a web service without any UI. c) Is not correct. The randomly generated values will not be visible for later investigation. d) Is not correct. Focused assertion messages can aid the investigation of the failures."
    },
    {
      "question_number": 27,
      "points": 3,
      "learning_objective": "TAE-6.1.2",
      "level": "K4",
      "question_text": "During the design phase of a performance critical software product, management decides to create a TAS to do load testing on the software to measure its performance. The product contains different architectural components, including a browser-based front end, microservices implemented in the back end, and a relational database. It is important to measure all individual transactions in the entire architectural stack. How can you automatically provide this information from the test automation software?",
      "options": {
        "a": "You cannot. This information has to be gathered manually at the end of every transaction",
        "b": "The test automation engineer (TAE) can record the timing information during the execution of all transactions",
        "c": "Trace IDs should be populated across the software components and measured time values should be associated with these IDs",
        "d": "Third-party tools should be inserted into the different layers of the architecture and these agents should log the collected data into a database"
      },
      "correct_answer": "c",
      "explanation": "a) Is not correct. This information cannot be gathered manually. b) Is not correct. This statement tells nothing about the technical details. c) Is correct. An ID must be introduced and populated across the architecture. d) Is not correct. There is no ID in this case, so measured data cannot be linked to individual transactions."
    },
    {
      "question_number": 28,
      "points": 1,
      "learning_objective": "TAE-6.1.3",
      "level": "K2",
      "question_text": "There is more than one series of test runs. Management wants to see if the test success rate is improving between test runs. What is a good way of fulfilling this requirement?",
      "options": {
        "a": "Compare the test results with the expected results",
        "b": "Traffic lights should indicate the progress of test execution",
        "c": "Detailed test reports with percentages of test completion",
        "d": "Introduce an analysis feature which takes the previous test results and highlights trends"
      },
      "correct_answer": "d",
      "explanation": "a) Is not correct. This is only for one test run. b) Is not correct. This is only for one test run. c) Is not correct. Percentages of test completion are irrelevant. d) Is correct. This is the correct implementation."
    },
    {
      "question_number": 29,
      "points": 2,
      "learning_objective": "TAE-7.1.1",
      "level": "K3",
      "question_text": "Your manager has requested that you develop a TAS for a computer-aided design (CAD) software program. This software has several different versions in production, and it has been ported to different languages and platforms. There are manual testing activities that must be automated. At the end of system testing, manual testers execute the software on their local computers and check the proper translation of the GUI elements. Each manual tester uses different language settings, versions, and platforms. As the first step in test automation, all testers should record the tests they have performed using a capture/playback tool. What might be an important consideration when designing this TAS?",
      "options": {
        "a": "Establish a central repository from which the same version of the capture/playback tool can be installed",
        "b": "Create and document an installation procedure of the CAD software",
        "c": "Set up a central test environment where the CAD software is installed and create an automation script which copies this environment to each local computer",
        "d": "Use configuration management for the dictionary files"
      },
      "correct_answer": "a",
      "explanation": "a) Is correct. There are several test environments, and a central repository is a must in this case to guarantee version consistency. b) Is not correct. This is not a TAS design consideration. c) Is not correct. A central test environment cannot be used because each tester has a different test environment. d) Is not correct. This option is not relevant."
    },
    {
      "question_number": 30,
      "points": 2,
      "learning_objective": "TAE-7.1.1",
      "level": "K3",
      "question_text": "You work in a software development team that requires testing to occur in many different test environments. Your manager has expressed that the team is spending a significant amount of time overcoming errors because the test automation solution (TAS) is not configured correctly when using it in a new test environment. Additionally, there appear to be version differences when comparing the TAS in each test environment. Even new test environments sometimes are set up with very old TAS components. The team is having a hard time keeping up with software changes and the test automation is not providing the value it should. Which options would help address this situation?",
      "options": {
        "a": "Create an automated installation script for the tools and configurations that make up the TAS",
        "b": "Limit the TAS to only be used in the most important test environments",
        "c": "Utilize a repository to store the TAS that is accessible to all test environments",
        "d": "Leverage manual testing to verify that the TAS has been configured properly in all test environments",
        "e": "Due to time constraints skip the implementation of components tests for the TAS"
      },
      "correct_answer": ["a", "c"],
      "explanation": "a) Is correct. Automated installation and configuration scripts ensure consistency and repeatability during TAS setup. b) Is not correct. The TAS should be designed for portability in multiple test environments. c) Is correct. Repositories can be used to verify consistent TAS versions across all test environments. d) Is not correct. Manual testing is not a scalable solution."
    },
    {
      "question_number": 31,
      "points": 1,
      "learning_objective": "TAE-7.1.2",
      "level": "K2",
      "question_text": "You are about to verify an automated test suite. During the verification process you have found that some test scripts pass one time and fail at another, therefore not providing reliable test results. What should you do to verify the validity of your test scripts?",
      "options": {
        "a": "This is due to the parallel execution of the test scripts; synchronization would solve the problem",
        "b": "Re-execute the automated test suite and analyze the test results again",
        "c": "Remove the test scripts from the automated test suite and analyze them separately",
        "d": "This happens because several test scripts are using the same test data, so the separation of test data for each test script would resolve the problem"
      },
      "correct_answer": "c",
      "explanation": "a) Is not correct. The issue should be analyzed closely since the root cause is not yet known. b) Is not correct. This answer does not suggest any solution. c) Is correct. This should be the first task in this case. d) Is not correct. The issue should be analyzed closely since the root cause is not yet known."
    },
    {
      "question_number": 32,
      "points": 1,
      "learning_objective": "TAE-7.1.2",
      "level": "K2",
      "question_text": "You have a test suite containing 25 automated tests that verify the login functionality of an application's home page. The test suite is executed at the end of each two-week sprint cycle for regression test purposes. You notice that two test cases out of the 25 may sometimes cause a race condition in the application or receive a random error. What action should you take for these two test cases?",
      "options": {
        "a": "Take no action because sometimes they execute successfully",
        "b": "Reduce the amount of test cases in the test suite from 25 to 15 and see if the test suite passes with the smaller amount",
        "c": "Remove the two test cases from the active test suite and analyze them separately to find the root cause",
        "d": "Replace the two test cases with ones that pass repeatedly so that the test suite still has 25 automated test cases"
      },
      "correct_answer": "c",
      "explanation": "a) Is not correct. The goal is to have repeatable tests that give consistent test results. b) Is not correct. Eliminating coverage is not a good response to the situation. c) Is correct. This allows the test suite to continue to be used for repeatable test results while providing time to triage problem test cases. They will be re-added to the test suite once corrected. d) Is not correct. This does not address the random errors that the application is experiencing."
    },
    {
      "question_number": 33,
      "points": 1,
      "learning_objective": "TAE-7.1.3",
      "level": "K2",
      "question_text": "You are working on a project to automate a regression test suite. When the regression test suite was executed manually last time, all the tests passed. But when you execute them via the test automation solution (TAS), you find there are some failed tests. What should you do to handle this situation?",
      "options": {
        "a": "Analyze log files to identify the root cause of the problem",
        "b": "Eliminate these test cases from the automated test suite, so the remaining tests can pass",
        "c": "Open a defect for the SUT as the failed tests are indicating an SUT problem",
        "d": "This is normal because automated tests behave differently than manual tests"
      },
      "correct_answer": "a",
      "explanation": "a) Is correct. Log file analysis can identify the root cause of the problem. b) Is not correct. This process will not help solve the original issue. c) Is not correct. These tests do not directly indicate a problem in the SUT; they should be analyzed first. d) Is not correct. The statement is false."
    },
    {
      "question_number": 34,
      "points": 1,
      "learning_objective": "TAE-7.1.4",
      "level": "K2",
      "question_text": "You are preparing to execute a test automation suite for a security-critical application which has to fulfill the highest security requirements. Which approach should you follow to verify the test automation code?",
      "options": {
        "a": "Search the test logs for possible credential data",
        "b": "Eliminate test cases using sensitive test data",
        "c": "Execute the test suite slowly and methodically to check if there are any security vulnerabilities",
        "d": "Use a static analysis tool to identify security vulnerabilities"
      },
      "correct_answer": "d",
      "explanation": "a) Is not correct. Test logs may not contain credential data present in the automation code. b) Is not correct. It does not make sense to eliminate test cases as it will affect code coverage. c) Is not correct. Slower test execution does not necessarily reveal security vulnerabilities. d) Is correct. Static analysis tools support identification of security vulnerabilities."
    },
    {
      "question_number": 35,
      "points": 2,
      "learning_objective": "TAE-8.1.1",
      "level": "K3",
      "question_text": "You are working on a test automation project that is used to automate graphical user interface (GUI) testing of an online web shop. The web shop contains a wizard which helps users set up their accounts: their names, billing address, shipping address and security credentials. Currently, the development of the software is in a phase when usability testers check the wizard and give recommendations about the necessary changes. This is done iteratively; the developers modify the GUI, the usability testers check the modifications, and repeat the usability testing. The test automation is mainly focusing on maintenance testing. In these UI-based test cases, data also includes UI locator values. An existing problem is that developers often change the internal identifiers of UI elements, so maintaining tests requires a lot of effort. Which of the following could be an important opportunity for improvement?",
      "options": {
        "a": "Apply schema validation, which checks if mandatory response elements are present on the GUI",
        "b": "Improve test logging to include information about UI elements and their locators to easily identify the broken test cases",
        "c": "Create a test histogram, which enables the TAEs to identify and select test cases that are fragile",
        "d": "Use an artificial intelligence (AI) algorithm - which is based on machine learning and image recognition - to identify the new selectors and use self-healing to fix the test cases"
      },
      "correct_answer": "d",
      "explanation": "a) Is not correct. Schema validation can be applied in API testing, not GUI testing. b) Is not correct. This is a manual and slow process. c) Is not correct. A test histogram enables identification of fragile test cases, but it does not solve the underlying problem. d) Is correct. Using an AI based algorithm supports identification of broken locators and self-healing of the test cases."
    },
    {
      "question_number": 36,
      "points": 2,
      "learning_objective": "TAE-8.1.1",
      "level": "K3",
      "question_text": "Your organization maintains a regression test suite of over 1000 automated test cases that has been extremely reliable over the years. Recently the development team has decided to modernize their technology stack and are currently refactoring how their front end operates. You notice that the application is far more API-driven than the previous version and this has an impact on how UI elements render. You anticipate this is going to impact the success rate of 75% of your automated test cases. What data analysis approaches should you use to determine how to fix your impacted automated test cases?",
      "options": {
        "a": "Run the test cases several times in a CI/CD pipeline, perform visual report analysis, and draw conclusions from a test histogram",
        "b": "Use AI algorithms and API schema validation tools",
        "c": "Recreate automated test cases to replace the ones that are not working properly and will execute on the new application",
        "d": "Avoid automating certain test cases after analyzing exception logs, screenshots, and error messages"
      },
      "correct_answer": "b",
      "explanation": "a) Is not correct. Analyzing test histograms for 1000 test cases will be time consuming. Plus, we can already anticipate the impact on test cases without generating histogram data. b) Is correct. AI algorithms can be used to self-heal the test cases against UI locator value changes, and schema validation tools can be used to quickly assess API schema updates. c) Is not correct. Recreating 75% of the test harness is not feasible when other options are available. d) Is not correct. Although logs, screenshots, and error messages are valid data sources to verify, eliminating test cases is not a viable strategy."
    },
    {
      "question_number": 37,
      "points": 3,
      "learning_objective": "TAE-8.1.2",
      "level": "K4",
      "question_text": "You are working on an automated regression test suite that takes too long a time to execute, and its execution does not finish overnight. The test environment is only available for regression testing during the night. Running multiple suites in parallel is not an option, as the target system is expensive and exists only as a single instance. What should be your next steps to ensure the test suite execution finishes overnight?",
      "options": {
        "a": "Split the test suite into multiple parts, executing the parts on different nights of the week",
        "b": "Isolate test result verification from the test execution and start the verification process after the test execution during morning hours",
        "c": "Rewrite the tests using the keyword-driven technique as that will be executed faster",
        "d": "Remove some tests from the test suite to reduce overall execution time",
        "e": "Remove any duplicate tests from the test suite"
      },
      "correct_answer": ["a", "e"],
      "explanation": "a) Is correct. Splitting up the test suite helps to ensure that the test execution is finished overnight. b) Is not correct. Test result verification cannot be isolated from the test execution process. c) Is not correct. It is not stated generally that the keyword-driven technique executes faster. d) Is not correct. It would reduce the scope to a high-level regression, which in the long term, could potentially result in defects propagating into production. e) Is correct. Removing duplications can reduce test execution time."
    },
    {
      "question_number": 38,
      "points": 3,
      "learning_objective": "TAE-8.1.2",
      "level": "K4",
      "question_text": "As a TAE, you are evaluating new versions of core libraries. Which is the correct order that can help you achieve these results?",
      "options": {
        "a": "Create adoption plan; determine impact; update dependencies; perform pilot",
        "b": "Perform pilot; determine impact; create adoption plan; update dependencies",
        "c": "Update dependencies; determine impact; perform pilot; create adoption plan",
        "d": "Determine impact; update dependencies; create adoption plan; perform pilot"
      },
      "correct_answer": "b",
      "explanation": "a) Is not correct. The adoption plan needs to occur after impact is determined. b) Is correct. This is the correct order of the activities. c) Is not correct. Updating of dependencies needs to occur after the creation of the adoption plan. d) Is not correct. Determining impact needs to occur after performing a pilot."
    },
    {
      "question_number": 39,
      "points": 2,
      "learning_objective": "TAE-8.1.3",
      "level": "K3",
      "question_text": "You have been performing a quality review of a TAS to optimize the interaction of controls within the GUI. The GUI includes several types of controls (e.g., dropdown list, checkbox, text field). There are also separate functions in the test scripts which act upon the different types of GUI controls to gather information and to set them (e.g., visible/not visible, enabled/not enabled). Which of the following steps should you consider to increase the efficiency of the TAS?",
      "options": {
        "a": "Separate the testing of the controls based on their types into different test suites",
        "b": "Research if there is a test automation tool which can replace the current solution",
        "c": "Check if there are any functions that can work with several types of controls, and consolidate the test scripts using these functions",
        "d": "Use the new operating system functions in the test scripts to handle the GUI controls"
      },
      "correct_answer": "c",
      "explanation": "a) Is not correct. Separating the testing of controls should be implemented within the core libraries. b) Is not correct. It is not a direct improvement. c) Is correct. It is an improvement to consolidate test scripts in this case. d) Is not correct. It is not a direct improvement."
    },
    {
      "question_number": 40,
      "points": 1,
      "learning_objective": "TAE-8.1.4",
      "level": "K2",
      "question_text": "For an automated performance test of a customer management system, it is needed to have customers with multiple profiles based on different data inputs as test data. What is the best way to implement such a solution?",
      "options": {
        "a": "Use a test automation tool to call a web service endpoint that registers these users and feeds in the data",
        "b": "Register these users manually via the GUI, so the GUI functionality can also be tested",
        "c": "Use the production database during the test as it has the real volume and type of data",
        "d": "Implement a test automation script to anonymize customer data before using it during the performance test"
      },
      "correct_answer": "a",
      "explanation": "a) Is correct. It offers an automated and quick solution to the problem. b) Is not correct. This is not a TAS. c) Is not correct. Using a production database directly as the source of the test data holds high risk. d) Is not correct. Anonymization of test data is important, but it is out of the scope of this solution."
    }
  ]
}